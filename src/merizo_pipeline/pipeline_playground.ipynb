{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 422,
   "id": "caf689c5-ba5d-4a1b-8264-8ffcee5db725",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.context.SparkContext"
      ]
     },
     "execution_count": 422,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "id": "b069b057-c1eb-4886-a8ec-3658d1819bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/almalinux/eda1-coursework/src/merizo_pipeline\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "id": "ad9edb22-c4a7-456e-9361-5d2a06a3f100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting torch==2.0.1+cpu\n",
      "  Using cached https://download.pytorch.org/whl/cpu/torch-2.0.1%2Bcpu-cp39-cp39-linux_x86_64.whl (195.4 MB)\n",
      "Requirement already satisfied: einops in /home/almalinux/.local/lib/python3.9/site-packages (from -r requirements.txt (line 2)) (0.8.0)\n",
      "Requirement already satisfied: matplotlib in /home/almalinux/.local/lib/python3.9/site-packages (from -r requirements.txt (line 3)) (3.9.3)\n",
      "Requirement already satisfied: natsort in /home/almalinux/.local/lib/python3.9/site-packages (from -r requirements.txt (line 4)) (8.4.0)\n",
      "Requirement already satisfied: networkx in /home/almalinux/.local/lib/python3.9/site-packages (from -r requirements.txt (line 5)) (3.2.1)\n",
      "Requirement already satisfied: numpy<2.0 in /home/almalinux/.local/lib/python3.9/site-packages (from -r requirements.txt (line 6)) (1.26.4)\n",
      "Requirement already satisfied: rotary_embedding_torch in /home/almalinux/.local/lib/python3.9/site-packages (from -r requirements.txt (line 7)) (0.8.6)\n",
      "Requirement already satisfied: scipy in /home/almalinux/.local/lib/python3.9/site-packages (from -r requirements.txt (line 8)) (1.13.1)\n",
      "Requirement already satisfied: setuptools in /home/almalinux/.local/lib/python3.9/site-packages (from -r requirements.txt (line 9)) (75.6.0)\n",
      "Requirement already satisfied: faiss-cpu in /home/almalinux/.local/lib/python3.9/site-packages (from -r requirements.txt (line 10)) (1.9.0.post1)\n",
      "Requirement already satisfied: fs in /home/almalinux/.local/lib/python3.9/site-packages (from -r requirements.txt (line 11)) (2.4.16)\n",
      "Requirement already satisfied: filelock in /usr/lib/python3.9/site-packages (from torch==2.0.1+cpu->-r requirements.txt (line 1)) (3.7.1)\n",
      "Requirement already satisfied: sympy in /home/almalinux/.local/lib/python3.9/site-packages (from torch==2.0.1+cpu->-r requirements.txt (line 1)) (1.13.1)\n",
      "Requirement already satisfied: typing-extensions in /home/almalinux/.local/lib/python3.9/site-packages (from torch==2.0.1+cpu->-r requirements.txt (line 1)) (4.12.2)\n",
      "Requirement already satisfied: jinja2 in /home/almalinux/.local/lib/python3.9/site-packages (from torch==2.0.1+cpu->-r requirements.txt (line 1)) (3.1.4)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/almalinux/.local/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 3)) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/almalinux/.local/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 3)) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /home/almalinux/.local/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 3)) (11.0.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/almalinux/.local/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 3)) (4.55.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/almalinux/.local/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 3)) (1.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/almalinux/.local/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 3)) (2.9.0.post0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/almalinux/.local/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 3)) (0.12.1)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /home/almalinux/.local/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 3)) (6.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/almalinux/.local/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 3)) (3.2.0)\n",
      "Requirement already satisfied: appdirs~=1.4.3 in /home/almalinux/.local/lib/python3.9/site-packages (from fs->-r requirements.txt (line 11)) (1.4.4)\n",
      "Requirement already satisfied: six~=1.10 in /usr/lib/python3.9/site-packages (from fs->-r requirements.txt (line 11)) (1.15.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /home/almalinux/.local/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib->-r requirements.txt (line 3)) (3.21.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/almalinux/.local/lib/python3.9/site-packages (from jinja2->torch==2.0.1+cpu->-r requirements.txt (line 1)) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/almalinux/.local/lib/python3.9/site-packages (from sympy->torch==2.0.1+cpu->-r requirements.txt (line 1)) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "id": "641a42f5-44ff-4dd9-bd26-15d4ca2e8148",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "5d39ee9c-bf03-46b1-a28e-c13d7ce81f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "id": "e5a3a6f4-8950-4783-93d9-e78f71e963fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'local[4]'"
      ]
     },
     "execution_count": 427,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "id": "1e0d8e77-7bf6-4d8b-80a3-f9cfc394d954",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/almalinux/eda1-coursework/src/merizo_pipeline'"
      ]
     },
     "execution_count": 428,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "e36d6a53-4605-40b0-8ea6-9316842075b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dir(input_dir, output_dir):\n",
    "    \"\"\"\n",
    "    Read file paths from HDFS using SparkContext.\n",
    "    \"\"\"\n",
    "    # Use SparkContext to read files from HDFS\n",
    "    # If files are located in a directory on HDFS, you can use textFile \n",
    "    file_rdd = sc.wholeTextFile(input_dir + \"/AF-Q46839-F1-model_v4.pdb\") \n",
    "    file_paths = file_rdd.collect()  # This retrieves the file paths as a list\n",
    "\n",
    "    # Create a list of tuples with file path, id, and output directory\n",
    "    return [(file_path, os.path.basename(file_path), output_dir) for file_path in file_paths]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "3dd95c37-f9f2-4ef7-b0a1-c52f56d85d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = \"/UP000000625_83333_ECOLI_v4/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "id": "d1b892af-2343-4be6-a3dd-11e91e1682ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_rdd = sc.wholeTextFiles(input_dir + \"*.pdb\")\n",
    "file_paths_rdd = file_rdd.map(lambda x: (x[0], os.path.basename(x[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "18a0b535-002c-4f75-b589-ee9ae132bb59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/UP000000625_83333_ECOLI_v4/AF-Q46839-F1-model_v4.pdb'"
      ]
     },
     "execution_count": 432,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkFiles\n",
    "local_file_path = SparkFiles.get(input_dir + \"/AF-Q46839-F1-model_v4.pdb\")\n",
    "local_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "id": "648a0de1-78d5-4366-9006-13aa180f7f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AF-P00811-F1-model_v4.pdb.parsed  remove_requirements.txt\n",
      "AF-P37095-F1-model_v4.pdb.parsed  requirements.txt\n",
      "AF-P39368-F1-model_v4.pdb.parsed  results_parser.py\n",
      "AF-P76079-F1-model_v4.pdb.parsed  setup.py\n",
      "merizo_pipeline\t\t\t  test.bin\n",
      "pipeline_job.py\t\t\t  test_requirements.txt\n",
      "pipeline_playground.ipynb\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "id": "6a6e4361-0e4d-4aee-b57e-b74cb3adb2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from subprocess import Popen, PIPE\n",
    "from tempfile import NamedTemporaryFile\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "id": "152a5137-8f0e-45c0-ae92-e77abb53fcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = \"/UP000000625_83333_ECOLI_v4/\"\n",
    "# input_dir = \"/\"\n",
    "# Failed example : AF-P0DSE5-F1-model_v4.pdb\n",
    "# Success example: AF-P75975-F1-model_v4.pdb\n",
    "file_rdd = sc.binaryFiles(input_dir + \"*.pdb\")\n",
    "file_rdd = file_rdd.sample(withReplacement=False, fraction=0.001)\n",
    "file_content_rdd = file_rdd.map(lambda x: (os.path.basename(x[0]), x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "id": "698eb083-3005-4730-b6b9-05500037a634",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_local_file(file_path):\n",
    "    try:\n",
    "        os.remove(file_path)\n",
    "        print(f\"{file_path} local file has been deleted.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"{file_path} does not exist.\")\n",
    "    except PermissionError:\n",
    "        print(f\"Permission denied to delete {file_path}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "id": "ba660d1e-703a-4bab-8b61-d4bbc0b5fff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_file_to_hdfs(local_file_path, hdfs_file_path):\n",
    "    hdfs_put_cmd = ['hdfs', 'dfs', '-put', local_file_path, hdfs_file_path]\n",
    "    print(f'STEP 3: UPLOADING ANALYSIS OUTPUT TO HDFS: {\" \".join(hdfs_put_cmd)}')\n",
    "    p = Popen(hdfs_put_cmd, stdin=PIPE,stdout=PIPE, stderr=PIPE)\n",
    "    out, err = p.communicate()\n",
    "    # Decode the byte output to string\n",
    "    print(\"Output:\")\n",
    "    print(out.decode(\"utf-8\"))  # Decode and print the standard output\n",
    "    \n",
    "    if err:\n",
    "        print(\"Error:\")\n",
    "        print(err.decode(\"utf-8\"))  # Decode and print the standard  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "id": "505990b7-0442-4bc1-8378-43f6273aa28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_parser(input_file):\n",
    "    \"\"\"\n",
    "    Run the results_parser.py over the hhr file to produce the output summary\n",
    "    \"\"\"\n",
    "    search_file = input_file+\"_search.tsv\"\n",
    "    print(\"search_file: \", search_file)\n",
    "    cmd = ['python3', './results_parser.py', search_file]\n",
    "    print(f'STEP 2: RUNNING PARSER: {\" \".join(cmd)}')\n",
    "    p = Popen(cmd, stdin=PIPE,stdout=PIPE, stderr=PIPE)\n",
    "    out, err = p.communicate()\n",
    "    # Decode the byte output to string\n",
    "    print(\"Output:\")\n",
    "    print(out.decode(\"utf-8\"))  # Decode and print the standard output\n",
    "        \n",
    "    if err:\n",
    "        print(\"Error:\")\n",
    "        print(err.decode(\"utf-8\"))  # Decode and print the standard error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "id": "04ecbfac-2f02-4380-91bb-ae208bb91aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_merizo_search(file_name, file_content):\n",
    "    print(f\"File Name: {file_name}\")\n",
    "    # Create a temporary file to hold the content\n",
    "    with NamedTemporaryFile(delete=True, mode='wb') as temp_file:\n",
    "        temp_file.write(file_content)\n",
    "        temp_file_path = temp_file.name\n",
    "        cmd = ['python3',\n",
    "           '/home/almalinux/merizo_search/merizo_search/merizo.py',\n",
    "           'easy-search',\n",
    "           temp_file_path,\n",
    "           '/home/almalinux/data/cath-4.3-foldclassdb',\n",
    "           file_name,\n",
    "           'tmp',\n",
    "           '--iterate',\n",
    "           '--output_headers',\n",
    "           '-d',\n",
    "           'cpu',\n",
    "           '--threads',\n",
    "           '2'\n",
    "          ]\n",
    "        print(f'STEP 1: RUNNING MERIZO: {\" \".join(cmd)}')\n",
    "        p = Popen(cmd, stdin=PIPE,stdout=PIPE, stderr=PIPE)\n",
    "        out, err = p.communicate()\n",
    "        # Decode the byte output to string\n",
    "        print(\"Output:\")\n",
    "        print(out.decode(\"utf-8\"))  # Decode and print the standard output\n",
    "        \n",
    "        if err:\n",
    "            print(\"Error:\")\n",
    "            print(err.decode(\"utf-8\"))  # Decode and print the standard \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "94a16936-dd7a-4ed6-8c45-dfb37b201e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fs.memoryfs import MemoryFS\n",
    "\n",
    "def run_merizo_search_vfs(file_name, file_content):\n",
    "    print(f\"File Name: {file_name}\")\n",
    "    # Initialize an in-memory virtual filesystem\n",
    "    virtual_fs = MemoryFS()\n",
    "    # Create a temporary file to hold the content\n",
    "    with virtual_fs.open(file_name, mode='wb') as virtual_file:\n",
    "        virtual_file.write(file_content)\n",
    "        if virtual_fs.exists(file_name):\n",
    "            print(f\"File '{file_name}' successfully written to the virtual filesystem.\")\n",
    "            with open(file_name, 'r') as pdb_file:\n",
    "                print(pdb_file.readlines())\n",
    "        # cmd = ['python3',\n",
    "        #    '/home/almalinux/merizo_search/merizo_search/merizo.py',\n",
    "        #    'easy-search',\n",
    "        #    file_name,\n",
    "        #    '/home/almalinux/data/cath-4.3-foldclassdb',\n",
    "        #    file_name,\n",
    "        #    'tmp',\n",
    "        #    '--iterate',\n",
    "        #    '--output_headers',\n",
    "        #    '-d',\n",
    "        #    'cpu',\n",
    "        #    '--threads',\n",
    "        #    '2'\n",
    "        #   ]\n",
    "        # print(f'STEP 1: RUNNING MERIZO: {\" \".join(cmd)}')\n",
    "        # p = Popen(cmd, stdin=PIPE,stdout=PIPE, stderr=PIPE)\n",
    "        # out, err = p.communicate()\n",
    "        # # Decode the byte output to string\n",
    "        # print(\"Output:\")\n",
    "        # print(out.decode(\"utf-8\"))  # Decode and print the standard output\n",
    "        \n",
    "        # if err:\n",
    "        #     print(\"Error:\")\n",
    "        #     print(err.decode(\"utf-8\"))  # Decode and print the standard \n",
    "    \n",
    "    # Clean up the virtual filesystem\n",
    "    virtual_fs.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "d86b4ff8-6eee-458b-bfc5-0deea896a475",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_analysis_outputs_to_hdfs(file_name):\n",
    "    # upload anaylsis output files to hdfs and clean local files\n",
    "    local_files_paths = [ file_name + '_segment.tsv', file_name + '_search.tsv', file_name + '.parsed']\n",
    "    hdfs_file_path = '/analysis_outputs/'\n",
    "    for local_file_path in local_files_paths:\n",
    "        upload_file_to_hdfs(local_file_path, hdfs_file_path)\n",
    "        delete_local_file(local_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7b36bf-21d3-43df-8cc9-a34a051d7ce2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "id": "4194e055-f2d1-4020-9c7d-77d708b76ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(file_tuple):\n",
    "    file_name, file_content = file_tuple\n",
    "    # STEP 1\n",
    "    run_merizo_search(file_name, file_content)\n",
    "    # STEP 2\n",
    "    run_parser(file_name)\n",
    "    # STEP 3\n",
    "    # parsed_output_dict = read_parsed_file(file_name)\n",
    "    upload_analysis_outputs_to_hdfs(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "id": "223d0f84-114e-4f8a-ab2a-6793077f602f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "File Name: AF-P0AGE6-F1-model_v4.pdb\n",
      "STEP 1: RUNNING MERIZO: python3 /home/almalinux/merizo_search/merizo_search/merizo.py easy-search /tmp/tmp765gvs3u /home/almalinux/data/cath-4.3-foldclassdb AF-P0AGE6-F1-model_v4.pdb tmp --iterate --output_headers -d cpu --threads 2\n",
      "File Name: AF-P51024-F1-model_v4.pdb\n",
      "STEP 1: RUNNING MERIZO: python3 /home/almalinux/merizo_search/merizo_search/merizo.py easy-search /tmp/tmp64f4ejqs /home/almalinux/data/cath-4.3-foldclassdb AF-P51024-F1-model_v4.pdb tmp --iterate --output_headers -d cpu --threads 2\n",
      "Output:\n",
      "\n",
      "Error:\n",
      "2024-12-19 11:42:25,262 | INFO | Starting easy-search with command: \n",
      "\n",
      "/home/almalinux/merizo_search/merizo_search/merizo.py easy-search /tmp/tmp765gvs3u /home/almalinux/data/cath-4.3-foldclassdb AF-P0AGE6-F1-model_v4.pdb tmp --iterate --output_headers -d cpu --threads 2\n",
      "\n",
      "2024-12-19 11:42:32,554 | INFO | Finished easy-search in 7.291 seconds.\n",
      "\n",
      "search_file:  AF-P0AGE6-F1-model_v4.pdb_search.tsv\n",
      "STEP 2: RUNNING PARSER: python3 ./results_parser.py AF-P0AGE6-F1-model_v4.pdb_search.tsv\n",
      "Output:\n",
      "\n",
      "STEP 3: UPLOADING ANALYSIS OUTPUT TO HDFS: hdfs dfs -put AF-P0AGE6-F1-model_v4.pdb_segment.tsv /analysis_outputs/\n",
      "Output:\n",
      "\n",
      "AF-P0AGE6-F1-model_v4.pdb_segment.tsv local file has been deleted.\n",
      "STEP 3: UPLOADING ANALYSIS OUTPUT TO HDFS: hdfs dfs -put AF-P0AGE6-F1-model_v4.pdb_search.tsv /analysis_outputs/\n",
      "Output:\n",
      "\n",
      "AF-P0AGE6-F1-model_v4.pdb_search.tsv local file has been deleted.\n",
      "STEP 3: UPLOADING ANALYSIS OUTPUT TO HDFS: hdfs dfs -put AF-P0AGE6-F1-model_v4.pdb.parsed /analysis_outputs/\n",
      "Output:\n",
      "\n",
      "AF-P0AGE6-F1-model_v4.pdb.parsed local file has been deleted.\n",
      "Output:\n",
      "\n",
      "Error:\n",
      "2024-12-19 11:42:33,181 | INFO | Starting easy-search with command: \n",
      "\n",
      "/home/almalinux/merizo_search/merizo_search/merizo.py easy-search /tmp/tmp64f4ejqs /home/almalinux/data/cath-4.3-foldclassdb AF-P51024-F1-model_v4.pdb tmp --iterate --output_headers -d cpu --threads 2\n",
      "\n",
      "2024-12-19 11:42:41,018 | INFO | Finished easy-search in 7.837 seconds.\n",
      "\n",
      "search_file:  AF-P51024-F1-model_v4.pdb_search.tsv\n",
      "STEP 2: RUNNING PARSER: python3 ./results_parser.py AF-P51024-F1-model_v4.pdb_search.tsv\n",
      "Output:\n",
      "\n",
      "STEP 3: UPLOADING ANALYSIS OUTPUT TO HDFS: hdfs dfs -put AF-P51024-F1-model_v4.pdb_segment.tsv /analysis_outputs/\n",
      "Output:\n",
      "\n",
      "AF-P51024-F1-model_v4.pdb_segment.tsv local file has been deleted.\n",
      "STEP 3: UPLOADING ANALYSIS OUTPUT TO HDFS: hdfs dfs -put AF-P51024-F1-model_v4.pdb_search.tsv /analysis_outputs/\n",
      "Output:\n",
      "\n",
      "AF-P51024-F1-model_v4.pdb_search.tsv local file has been deleted.\n",
      "STEP 3: UPLOADING ANALYSIS OUTPUT TO HDFS: hdfs dfs -put AF-P51024-F1-model_v4.pdb.parsed /analysis_outputs/\n",
      "File Name: AF-P23878-F1-model_v4.pdb\n",
      "STEP 1: RUNNING MERIZO: python3 /home/almalinux/merizo_search/merizo_search/merizo.py easy-search /tmp/tmp2myyl_cw /home/almalinux/data/cath-4.3-foldclassdb AF-P23878-F1-model_v4.pdb tmp --iterate --output_headers -d cpu --threads 2\n",
      "Output:\n",
      "\n",
      "AF-P51024-F1-model_v4.pdb.parsed local file has been deleted.\n",
      "Output:\n",
      "\n",
      "Error:\n",
      "2024-12-19 11:42:49,890 | INFO | Starting easy-search with command: \n",
      "\n",
      "/home/almalinux/merizo_search/merizo_search/merizo.py easy-search /tmp/tmp2myyl_cw /home/almalinux/data/cath-4.3-foldclassdb AF-P23878-F1-model_v4.pdb tmp --iterate --output_headers -d cpu --threads 2\n",
      "\n",
      "2024-12-19 11:42:59,370 | INFO | Finished easy-search in 9.479 seconds.\n",
      "\n",
      "search_file:  AF-P23878-F1-model_v4.pdb_search.tsv\n",
      "STEP 2: RUNNING PARSER: python3 ./results_parser.py AF-P23878-F1-model_v4.pdb_search.tsv\n",
      "Output:\n",
      "\n",
      "STEP 3: UPLOADING ANALYSIS OUTPUT TO HDFS: hdfs dfs -put AF-P23878-F1-model_v4.pdb_segment.tsv /analysis_outputs/\n",
      "Output:\n",
      "\n",
      "AF-P23878-F1-model_v4.pdb_segment.tsv local file has been deleted.\n",
      "STEP 3: UPLOADING ANALYSIS OUTPUT TO HDFS: hdfs dfs -put AF-P23878-F1-model_v4.pdb_search.tsv /analysis_outputs/\n",
      "Output:\n",
      "\n",
      "AF-P23878-F1-model_v4.pdb_search.tsv local file has been deleted.\n",
      "STEP 3: UPLOADING ANALYSIS OUTPUT TO HDFS: hdfs dfs -put AF-P23878-F1-model_v4.pdb.parsed /analysis_outputs/\n",
      "Output:\n",
      "\n",
      "AF-P23878-F1-model_v4.pdb.parsed local file has been deleted.\n",
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None]"
      ]
     },
     "execution_count": 441,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_content_rdd.map(pipeline).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "id": "96fa9a33-c56f-4372-9678-9811225ded46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#AF-P00811-F1-model_v4.pdb_search.tsv Results. mean plddt: 98.4244\n",
      "cath_id,count\n",
      "3.40.710.10,1\n"
     ]
    }
   ],
   "source": [
    "!cat AF-P00811-F1-model_v4.pdb.parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "id": "f2e83bb7-667f-4b5b-9871-78bde7b3cae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_str = \"#AF-P00811-F1-model_v4.pdb_search.tsv Results. mean plddt: 98.4244\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "id": "95a52974-e348-4b4c-b8f6-cef9cb8f6aa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98.4244"
      ]
     },
     "execution_count": 444,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(test_str.split(\"mean plddt: \")[1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "id": "60ad4b69-2b01-45c9-9da7-3272ab9e323f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'3.40.710.10': 1}, 98.4244)"
      ]
     },
     "execution_count": 445,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def parse_parsed_file(file_path):\n",
    "    \"\"\"\n",
    "    Reads a .parsed file and extracts:\n",
    "    1. A dictionary with 'cath_id' as keys and their counts as values.\n",
    "    2. The mean pLDDT value.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the .parsed file.\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (dict of cath_id counts, mean pLDDT value)\n",
    "    \"\"\"\n",
    "    cath_counts = {}\n",
    "    mean_plddt = 0\n",
    "    \n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        \n",
    "        # Extract mean pLDDT from the header\n",
    "        if len(lines) < 1:\n",
    "            return cath_counts, mean_plddt\n",
    "\n",
    "        first_line = lines[0]\n",
    "        if \"mean plddt:\" in first_line:\n",
    "            mean_plddt = float(first_line.split(\"mean plddt:\")[1].strip())\n",
    "        \n",
    "        # Skip the header and process the data rows\n",
    "        for line in lines[2:]:  # Assuming data rows start from the 3rd line\n",
    "            if not line.strip():\n",
    "                continue # Ignore empty lines\n",
    "            cath_id, count = line.strip().split(',')\n",
    "            cath_counts[cath_id] = int(count)\n",
    "    \n",
    "    return cath_counts, mean_plddt\n",
    "\n",
    "# Apply the function to the uploaded file\n",
    "file_path = \"AF-P00811-F1-model_v4.pdb.parsed\"\n",
    "cath_counts, mean_plddt = parse_parsed_file(file_path)\n",
    "cath_counts, mean_plddt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "id": "347e31c6-009d-4921-8841-7088c7e427e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'3.40.710.10': 4, '2.30.810.20': 2}, 93.4244)"
      ]
     },
     "execution_count": 446,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def combine_parsed_dict_and_mean(analysis_output1, analysis_output2):\n",
    "    cath_counts_dict1, mean_plddt1 = analysis_output1\n",
    "    cath_counts_dict2, mean_plddt2 = analysis_output2\n",
    "\n",
    "    for key in cath_counts_dict2:\n",
    "        if key in cath_counts_dict1:\n",
    "            cath_counts_dict1[key] +=cath_counts_dict2[key]\n",
    "        else:\n",
    "            cath_counts_dict1[key] = cath_counts_dict2[key]\n",
    "\n",
    "    mean_plddt = (mean_plddt1 + mean_plddt2) / 2.0\n",
    "\n",
    "    return cath_counts_dict1, mean_plddt\n",
    "\n",
    "analysis_output1 = ({'3.40.710.10': 1}, 98.4244)\n",
    "analysis_output2 = ({'3.40.710.10': 3, '2.30.810.20': 2}, 88.4244)\n",
    "\n",
    "combine_parsed_dict_and_mean(analysis_output1, analysis_output2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba44418e-d0c2-4d95-9f9b-22b780f91b98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "id": "f0d78076-4299-4153-8384-015c00c0a0b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'virtual_test.bin' successfully written to the virtual filesystem.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'virtual_test.bin'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[447], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m virtual_fs\u001b[38;5;241m.\u001b[39mexists(virtual_file_path):\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvirtual_file_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m successfully written to the virtual filesystem.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 21\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvirtual_file_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m     22\u001b[0m         \u001b[38;5;28mprint\u001b[39m(file\u001b[38;5;241m.\u001b[39mreadlines())\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/IPython/core/interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    308\u001b[0m     )\n\u001b[0;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'virtual_test.bin'"
     ]
    }
   ],
   "source": [
    "from fs.memoryfs import MemoryFS\n",
    "\n",
    "# Initialize an in-memory virtual filesystem\n",
    "virtual_fs = MemoryFS()\n",
    "\n",
    "# Path of the binary file to read\n",
    "binary_file_path = 'test.bin'\n",
    "\n",
    "# Path to write the file in the virtual filesystem\n",
    "virtual_file_path = 'virtual_test.bin'\n",
    "\n",
    "# Read the binary file and write its content to the virtual filesystem\n",
    "with open(binary_file_path, 'rb') as local_file:\n",
    "    binary_content = local_file.read()\n",
    "    with virtual_fs.open(virtual_file_path, 'wb') as virtual_file:\n",
    "        virtual_file.write(binary_content)\n",
    "\n",
    "# Verify the file exists in the virtual filesystem\n",
    "if virtual_fs.exists(virtual_file_path):\n",
    "    print(f\"File '{virtual_file_path}' successfully written to the virtual filesystem.\")\n",
    "    with open(virtual_file_path, 'r') as file:\n",
    "        print(file.readlines())\n",
    "else:\n",
    "    print(\"Failed to write the file.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Clean up the virtual filesystem\n",
    "# virtual_fs.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "id": "6f6d2f1e-d424-4e8c-b9c5-b76f78ff7fa2",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/tmp/spark-7b6032d9-bfc3-4f74-8bd8-8040d65ec3b4/pyspark-d645a6d8-e77e-4674-9d32-f886bf77eb95/tmp3tvlpmna'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[448], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43msc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallelize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/spark-3.5.3-bin-hadoop3-scala2.13/python/pyspark/context.py:824\u001b[0m, in \u001b[0;36mSparkContext.parallelize\u001b[0;34m(self, c, numSlices)\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    822\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonParallelizeServer(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc\u001b[38;5;241m.\u001b[39msc(), numSlices)\n\u001b[0;32m--> 824\u001b[0m jrdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_serialize_to_jvm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserializer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreader_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreateRDDServer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m RDD(jrdd, \u001b[38;5;28mself\u001b[39m, serializer)\n",
      "File \u001b[0;32m~/spark-3.5.3-bin-hadoop3-scala2.13/python/pyspark/context.py:864\u001b[0m, in \u001b[0;36mSparkContext._serialize_to_jvm\u001b[0;34m(self, data, serializer, reader_func, server_func)\u001b[0m\n\u001b[1;32m    860\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m r\n\u001b[1;32m    861\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    862\u001b[0m     \u001b[38;5;66;03m# without encryption, we serialize to a file, and we read the file in java and\u001b[39;00m\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;66;03m# parallelize from there.\u001b[39;00m\n\u001b[0;32m--> 864\u001b[0m     tempFile \u001b[38;5;241m=\u001b[39m \u001b[43mNamedTemporaryFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelete\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdir\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_temp_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    865\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    866\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib64/python3.9/tempfile.py:561\u001b[0m, in \u001b[0;36mNamedTemporaryFile\u001b[0;34m(mode, buffering, encoding, newline, suffix, prefix, dir, delete, errors)\u001b[0m\n\u001b[1;32m    558\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _os\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnt\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m delete:\n\u001b[1;32m    559\u001b[0m     flags \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m _os\u001b[38;5;241m.\u001b[39mO_TEMPORARY\n\u001b[0;32m--> 561\u001b[0m (fd, name) \u001b[38;5;241m=\u001b[39m \u001b[43m_mkstemp_inner\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mdir\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuffix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    563\u001b[0m     file \u001b[38;5;241m=\u001b[39m _io\u001b[38;5;241m.\u001b[39mopen(fd, mode, buffering\u001b[38;5;241m=\u001b[39mbuffering,\n\u001b[1;32m    564\u001b[0m                     newline\u001b[38;5;241m=\u001b[39mnewline, encoding\u001b[38;5;241m=\u001b[39mencoding, errors\u001b[38;5;241m=\u001b[39merrors)\n",
      "File \u001b[0;32m/usr/lib64/python3.9/tempfile.py:255\u001b[0m, in \u001b[0;36m_mkstemp_inner\u001b[0;34m(dir, pre, suf, flags, output_type)\u001b[0m\n\u001b[1;32m    253\u001b[0m _sys\u001b[38;5;241m.\u001b[39maudit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtempfile.mkstemp\u001b[39m\u001b[38;5;124m\"\u001b[39m, file)\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 255\u001b[0m     fd \u001b[38;5;241m=\u001b[39m \u001b[43m_os\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0o600\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileExistsError\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m    \u001b[38;5;66;03m# try again\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/tmp/spark-7b6032d9-bfc3-4f74-8bd8-8040d65ec3b4/pyspark-d645a6d8-e77e-4674-9d32-f886bf77eb95/tmp3tvlpmna'"
     ]
    }
   ],
   "source": [
    "results = sc.parallelize([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "id": "2290bf92-6cf8-4c16-91ac-e0bf7f075fb0",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/tmp/spark-7b6032d9-bfc3-4f74-8bd8-8040d65ec3b4/pyspark-d645a6d8-e77e-4674-9d32-f886bf77eb95/tmp_u6_n6jk'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[417], line 49\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m combined_dict, overall_mean, std_dev\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Simulate the map step\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m parsed_results \u001b[38;5;241m=\u001b[39m \u001b[43msc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallelize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCATH1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCATH2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m80.0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCATH1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCATH3\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m75.0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCATH2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCATH3\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m85.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Initial values: (empty dict, (sum_means, count, sum_squared_diffs))\u001b[39;00m\n\u001b[1;32m     56\u001b[0m initial_value \u001b[38;5;241m=\u001b[39m ({}, (\u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0.0\u001b[39m))\n",
      "File \u001b[0;32m~/spark-3.5.3-bin-hadoop3-scala2.13/python/pyspark/context.py:824\u001b[0m, in \u001b[0;36mSparkContext.parallelize\u001b[0;34m(self, c, numSlices)\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    822\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonParallelizeServer(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc\u001b[38;5;241m.\u001b[39msc(), numSlices)\n\u001b[0;32m--> 824\u001b[0m jrdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_serialize_to_jvm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserializer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreader_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreateRDDServer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m RDD(jrdd, \u001b[38;5;28mself\u001b[39m, serializer)\n",
      "File \u001b[0;32m~/spark-3.5.3-bin-hadoop3-scala2.13/python/pyspark/context.py:864\u001b[0m, in \u001b[0;36mSparkContext._serialize_to_jvm\u001b[0;34m(self, data, serializer, reader_func, server_func)\u001b[0m\n\u001b[1;32m    860\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m r\n\u001b[1;32m    861\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    862\u001b[0m     \u001b[38;5;66;03m# without encryption, we serialize to a file, and we read the file in java and\u001b[39;00m\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;66;03m# parallelize from there.\u001b[39;00m\n\u001b[0;32m--> 864\u001b[0m     tempFile \u001b[38;5;241m=\u001b[39m \u001b[43mNamedTemporaryFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelete\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdir\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_temp_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    865\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    866\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib64/python3.9/tempfile.py:561\u001b[0m, in \u001b[0;36mNamedTemporaryFile\u001b[0;34m(mode, buffering, encoding, newline, suffix, prefix, dir, delete, errors)\u001b[0m\n\u001b[1;32m    558\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _os\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnt\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m delete:\n\u001b[1;32m    559\u001b[0m     flags \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m _os\u001b[38;5;241m.\u001b[39mO_TEMPORARY\n\u001b[0;32m--> 561\u001b[0m (fd, name) \u001b[38;5;241m=\u001b[39m \u001b[43m_mkstemp_inner\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mdir\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuffix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    563\u001b[0m     file \u001b[38;5;241m=\u001b[39m _io\u001b[38;5;241m.\u001b[39mopen(fd, mode, buffering\u001b[38;5;241m=\u001b[39mbuffering,\n\u001b[1;32m    564\u001b[0m                     newline\u001b[38;5;241m=\u001b[39mnewline, encoding\u001b[38;5;241m=\u001b[39mencoding, errors\u001b[38;5;241m=\u001b[39merrors)\n",
      "File \u001b[0;32m/usr/lib64/python3.9/tempfile.py:255\u001b[0m, in \u001b[0;36m_mkstemp_inner\u001b[0;34m(dir, pre, suf, flags, output_type)\u001b[0m\n\u001b[1;32m    253\u001b[0m _sys\u001b[38;5;241m.\u001b[39maudit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtempfile.mkstemp\u001b[39m\u001b[38;5;124m\"\u001b[39m, file)\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 255\u001b[0m     fd \u001b[38;5;241m=\u001b[39m \u001b[43m_os\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0o600\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileExistsError\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m    \u001b[38;5;66;03m# try again\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/tmp/spark-7b6032d9-bfc3-4f74-8bd8-8040d65ec3b4/pyspark-d645a6d8-e77e-4674-9d32-f886bf77eb95/tmp_u6_n6jk'"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def combine_results(acc, new_data):\n",
    "    \"\"\"\n",
    "    Combines two tuples of (cath_counts, mean_pLDDT_stats) into one.\n",
    "    \n",
    "    Args:\n",
    "        acc (tuple): Accumulated results (combined_dict, (sum_means, count, sum_squared_diffs)).\n",
    "        new_data (tuple): New data to combine (dict, mean_pLDDT).\n",
    "        \n",
    "    Returns:\n",
    "        tuple: Combined results.\n",
    "    \"\"\"\n",
    "    combined_dict, mean_stats = acc\n",
    "    new_dict, new_mean = new_data\n",
    "    \n",
    "    # Combine dictionaries by summing counts\n",
    "    for key, value in new_dict.items():\n",
    "        if key in combined_dict:\n",
    "            combined_dict[key] += value\n",
    "        else:\n",
    "            combined_dict[key] = value\n",
    "    \n",
    "    # Update mean stats\n",
    "    sum_means, count, sum_squared_diffs = mean_stats\n",
    "    count += 1\n",
    "    sum_means += new_mean\n",
    "    new_mean_diff_squared = (new_mean - (sum_means / count)) ** 2\n",
    "    sum_squared_diffs += new_mean_diff_squared\n",
    "    \n",
    "    return combined_dict, (sum_means, count, sum_squared_diffs)\n",
    "\n",
    "def finalize_results(combined_results):\n",
    "    \"\"\"\n",
    "    Finalizes the combined results to calculate the overall mean and standard deviation.\n",
    "    \n",
    "    Args:\n",
    "        combined_results (tuple): (combined_dict, (sum_means, count, sum_squared_diffs)).\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (combined_dict, overall_mean, std_dev)\n",
    "    \"\"\"\n",
    "    combined_dict, (sum_means, count, sum_squared_diffs) = combined_results\n",
    "    overall_mean = sum_means / count\n",
    "    std_dev = math.sqrt(sum_squared_diffs / count)\n",
    "    return combined_dict, overall_mean, std_dev\n",
    "\n",
    "# Simulate the map step\n",
    "parsed_results = sc.parallelize([\n",
    "    ({\"CATH1\": 10, \"CATH2\": 5}, 80.0),\n",
    "    ({\"CATH1\": 20, \"CATH3\": 15}, 75.0),\n",
    "    ({\"CATH2\": 8, \"CATH3\": 12}, 85.0)\n",
    "])\n",
    "\n",
    "# Initial values: (empty dict, (sum_means, count, sum_squared_diffs))\n",
    "initial_value = ({}, (0.0, 0, 0.0))\n",
    "\n",
    "# Reduce step\n",
    "combined_results = parsed_results.reduce(combine_results)\n",
    "\n",
    "# Finalize results to compute overall mean and standard deviation\n",
    "final_result = finalize_results(combined_results)\n",
    "print(\"Final Result:\", final_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9972785-b07a-4a23-bc7e-790750155da6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
